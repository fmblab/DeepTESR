{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"DeepTESR_onlyseq_210614.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"mlLpDXy0c59n"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import tensorflow as tf\n","\n","from sklearn.metrics import r2_score\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3QMOGGg-c59p"},"source":["# onehot encoding script for TESR sequence data set\n","def onehot(seq):\n","        module = np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]])\n","        i = 0\n","        onehot_result = []\n","        while i < len(seq):\n","            seqlist = []\n","            for base in seq[i]:\n","                if base == 'a' or base == 'A':\n","                    seqlist.append(module[0])\n","                elif base == 't' or base == 'T':\n","                    seqlist.append(module[1])\n","                elif base == 'g' or base == 'G':\n","                    seqlist.append(module[2])\n","                elif base == 'c' or base == 'C':\n","                    seqlist.append(module[3])\n","                else:\n","                    seqlist.append([0,0,0,0])\n","            onehot_result.append(seqlist)\n","            i = i + 1\n","        result = np.zeros((len(seq),9,1,4))\n","        result = np.float32(result)\n","        i = 0\n","        while i < len(seq):\n","            j = 0\n","            while j < len(seq[0]):\n","                result[i,j,0,:] = onehot_result[i][j]\n","                j = j + 1\n","            i = i + 1\n","        \n","        return result\n","\n","# Matching the codon usages for each TESR sequence with codon usage data file\n","def usage_matcher(path, sheet_name, host):    \n","    usage_data = pd.read_excel(\"c:/Users/FMB/Documents/python/MLDL/CDSanalysis/codon_usage.xlsx\", \n","                               sheet_name = host)\n","    codon_list = usage_data['codon']\n","    usage = usage_data.set_index('codon').to_dict(orient = 'index')\n","    \n","    sequence = pd.read_excel(path, sheet_name = sheet_name)\n","    seq_list = sequence[\"sequence\"]\n","    \n","    first_usage = []\n","    second_usage = []\n","    third_usage = []\n","    \n","    for i in range(len(seq_list)):\n","        first_codon = seq_list[i][0:3]\n","        second_codon = seq_list[i][3:6]\n","        third_codon = seq_list[i][6:9]\n","        first = usage[first_codon]['frequency']\n","        second = usage[second_codon]['frequency']\n","        third = usage[third_codon]['frequency']\n","        first_usage.append(first)\n","        second_usage.append(second)\n","        third_usage.append(third)\n","        \n","    df_first = pd.DataFrame(first_usage)\n","    df_second = pd.DataFrame(second_usage)\n","    df_third = pd.DataFrame(third_usage)\n","    \n","    result = pd.concat([sequence[[\"sequence\", \"score avg\"]], df_first, df_second, df_third ],\n","                       axis = 1, ignore_index=True)\n","    \n","    return result\n","\n","# codon usage min-max scaler\n","def Usage_MinMaxscaler(df):\n","    usage_1st = df['1st_usage'].values\n","    usage_1st = usage_1st.reshape(-1,1)\n","    \n","    usage_2nd = df['2nd_usage'].values\n","    usage_2nd = usage_2nd.reshape(-1,1)\n","    \n","    usage_3rd = df['3rd_usage'].values\n","    usage_3rd = usage_3rd.reshape(-1,1)\n","    \n","    scaler = MinMaxScaler()\n","    \n","    scaled_1 = pd.DataFrame(scaler.fit_transform(usage_1st))\n","    scaled_2 = pd.DataFrame(scaler.fit_transform(usage_2nd))\n","    scaled_3 = pd.DataFrame(scaler.fit_transform(usage_3rd))\n","    \n","    scaled_result = pd.concat([scaled_1, scaled_2, scaled_3], axis = 1)\n","    \n","    scaled_result.columns = ['1st_usage', '2nd_usage', '3rd_usage']\n","    \n","    return scaled_result\n","\n","# Separate and process the feature and label data from dataframe of training dataset.\n","def Data_processing(df):\n","    \n","    sequence_array = onehot(df['sequence'])\n","    \n","    label = df['score avg']\n","    \n","    usage_df = df[['1st_usage', '2nd_usage', '3rd_usage']]\n","    \n","    usage_array = Usage_MinMaxscaler(usage_df)\n","    \n","    return sequence_array, label, usage_array\n","\n","\n","def evaluate_model(trainX_seq, trainy, testX_seq, testy):\n","    \n","    input_seq = keras.Input(shape = (9,1,4,), name = 'X_train_seq_hot')\n","    y = keras.layers.Conv2D(128, (3, 1), activation = 'relu', padding = 'same')(input_seq)\n","    y = keras.layers.Conv2D(128, (3, 1), activation = 'relu', padding = 'same')(y)\n","    y = keras.layers.MaxPooling2D(pool_size = (4,1))(y)\n","    y = keras.layers.Conv2D(256, (2, 1), activation = 'relu', padding = 'same')(y)\n","    y = keras.layers.Conv2D(256, (2, 1), activation = 'relu', padding = 'same')(y)\n","    y = keras.layers.MaxPooling2D(pool_size = (2,1))(y)\n","    y = keras.layers.Flatten()(y)\n","    output_seq = keras.layers.Dense(9, activation = 'relu')(y)\n","    \n","    z = keras.layers.Dense(1024, activation = 'relu')(output_seq)\n","    z = keras.layers.Dense(512, activation = 'relu')(z)\n","    z = keras.layers.Dense(256, activation = 'relu')(z)\n","    z = keras.layers.BatchNormalization()(z)\n","    z = keras.layers.Dense(64, activation = 'relu')(z)\n","    z = keras.layers.Dense(16, activation = 'relu')(z)\n","    z = keras.layers.Dense(1, activation = 'linear')(z)\n","    \n","    \n","    model = keras.Model(inputs = input_seq, outputs = z)\n","    model.compile(loss = 'mean_absolute_error', optimizer = 'adam', metrics = ['mean_absolute_error'])\n","    \n","    MODEL_DIR = './model_Func/'\n","    if not os.path.exists(MODEL_DIR):\n","        os.mkdir(MODEL_DIR)\n","    \n","    model_save_path = \"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n","    checkpointer = ModelCheckpoint(filepath=model_save_path, monitor='val_loss', \n","                                   verbose=0, save_best_only=True)\n","    early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience=10)\n","    \n","    print(\"Training model...\")\n","\n","    history = model.fit(trainX_seq, trainy,\n","                        epochs = 100, batch_size = 64, validation_split = 0.9,\n","                        verbose = 0, callbacks = [early_stopping_callback,checkpointer])\n","    \n","    test_MAE, _ = model.evaluate(testX_seq, testy, verbose=0)\n","\n","    return model, test_MAE\n","\n","# Ensemble prediction with model set\n","def ensemble_predictions(model_set, testX_seq):\n","    \n","    yhats = [model.predict(testX_seq) for model in model_set]\n","    yhats = np.array(yhats)\n","    \n","    result = np.sum(yhats, axis=0) / len(model_set)\n","    return result\n","\n","# Partial evaluation of ensemble set\n","def partial_evaluation(model_set, number, testX_seq, testy):\n","    \n","    model_subset = model_set[:number]\n","    \n","    yhat = ensemble_predictions(model_subset,testX_seq)\n","    \n","    return mean_absolute_error(testy, yhat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJL07uizc59s"},"source":["# Loading and processing training data\n","\n","Total_dataset = usage_matcher(\"C:/Users/FMB/Documents/python/MLDL/Ecoli codon variant transformer.xlsx\",\n","                             \"score 1\", \"Ecoli\")\n","Total_dataset.columns = ['sequence', 'score avg', '1st_usage', '2nd_usage', '3rd_usage']\n","\n","Total_array, Total_label, Total_usage = Data_processing(Total_dataset)\n","\n","print(Total_dataset.head())\n","print(Total_array.shape)\n","print(Total_label.head())\n","print(Total_usage.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k97_c0Lec59u"},"source":["# Train - Test data split for model training(train : test = 9 : 1)\n","trainX_seq, testX_seq, trainX_usage, testX_usage, trainy, testy = train_test_split(Total_array, \n","                                                                                   Total_usage, Total_label, test_size = 0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EBLkyn-yc59u"},"source":["# Training of single model with same architecture of evaluate_model function\n","\n","input_seq = keras.Input(shape = (9,1,4,), name = 'X_train_seq_hot')\n","y = keras.layers.Conv2D(128, (3, 1), activation = 'relu', padding = 'same')(input_seq)\n","y = keras.layers.Conv2D(128, (3, 1), activation = 'relu', padding = 'same')(y)\n","y = keras.layers.MaxPooling2D(pool_size = (4,1))(y)\n","y = keras.layers.Conv2D(256, (2, 1), activation = 'relu', padding = 'same')(y)\n","y = keras.layers.Conv2D(256, (2, 1), activation = 'relu', padding = 'same')(y)\n","y = keras.layers.MaxPooling2D(pool_size = (2,1))(y)\n","y = keras.layers.Flatten()(y)\n","output_seq = keras.layers.Dense(9, activation = 'relu')(y)\n","\n","z = keras.layers.Dense(1024, activation = 'relu')(output_seq)\n","z = keras.layers.Dense(512, activation = 'relu')(z)\n","z = keras.layers.Dense(256, activation = 'relu')(z)\n","z = keras.layers.BatchNormalization()(z)\n","z = keras.layers.Dense(64, activation = 'relu')(z)\n","z = keras.layers.Dense(16, activation = 'relu')(z)\n","z = keras.layers.Dense(1, activation = 'linear')(z)\n","\n","\n","model = keras.Model(inputs = input_seq, outputs = z, name = 'sDeepTESR')\n","model.compile(loss = 'mean_absolute_error', optimizer = 'adam', metrics = ['mean_absolute_error'])\n","model.summary()\n","\n","MODEL_DIR = './model_Func/'\n","if not os.path.exists(MODEL_DIR):\n","    os.mkdir(MODEL_DIR)\n","    \n","modelpath = \"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n","checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n","early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience=10)\n","\n","print(\"Training model...\")\n","\n","history = model.fit(trainX_seq, trainy, \n","                    epochs = 100, batch_size = 64, validation_split = 0.9,\n","                    verbose = 1, callbacks = [early_stopping_callback,checkpointer])\n","y_vloss = history.history['val_loss']\n","y_err = history.history['mean_absolute_error']\n","x_len = np.arange(len(y_err))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ljMe7iCc59u"},"source":["# Visualization of training result from sDeepTESR\n","\n","plt.figure(figsize = (7,7), dpi = 300)\n","plt.plot(x_len[:20], y_vloss[:20], \"o\", linestyle = \"solid\", c=\"red\", markersize=0, label = \"validation loss\")\n","plt.plot(x_len[:20], y_err[:20], \"o\", linestyle = \"solid\", c=\"blue\", markersize=0, label = \"training loss\")\n","plt.legend(loc = (0.55, 0.83), fontsize = 15)\n","plt.title(\"sDeepTESR training result\", fontsize = 15)\n","plt.xlabel(\"Epochs\", fontsize = 15)\n","plt.xticks([0, 5, 10, 15, 20])\n","plt.ylabel(\"Mean absolute error\", fontsize = 15)\n","plt.savefig(\"C:/Users/FMB/Desktop/Training_result_only_seq_TESR.jpg\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hV6F62Rac59v"},"source":["# Visulization of test result from sDeepTESR\n","Test_Predict = model.predict(testX_seq)\n","test_loss, test_acc = model.evaluate(testX_seq, testy)\n","\n","plt.figure(figsize = (5,5), dpi = 300)\n","plt.scatter(testy, Test_Predict, alpha = 0.03)\n","plt.title(\"GFP score(Test set) - TESR score(Prediction)\", fontsize = 10)\n","plt.xlabel(\"GFP score(Test set)\", fontsize = 10)\n","plt.ylabel(\"TESR score(Prediction)\", fontsize = 10)\n","plt.xlim(1.0, 5.0)\n","plt.xticks([1, 2, 3, 4, 5])\n","plt.ylim(1.0, 5.0)\n","plt.yticks([1, 2, 3, 4, 5])\n","plt.savefig(\"C:/Users/FMB/Desktop/GFP_TESR_sDeepTESR_only_seq.jpg\")\n","plt.show()\n","\n","print(\"r2_score = \", r2_score(testy, Test_Predict))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"QnQmD8AOc59v"},"source":["# Bagging ensemble training for eDeepTESR\n","\n","n_splits = 20\n","scores, trained_models = list(), list()\n","\n","for _ in range(n_splits):\n","    print(\"train-test split...\")\n","    train_ens_usage, test_ens_usage, train_ens_seq, test_ens_seq, train_ens_y, test_ens_y = train_test_split(trainX_usage, trainX_seq, trainy, \n","                                                                                                             test_size = 0.2)\n","    # evaluate model\n","    model, test_MAE = evaluate_model(train_ens_usage, train_ens_seq, train_ens_y, \n","                                     test_ens_usage, test_ens_seq, test_ens_y)\n","    print('Mean_absolute_error=%.5f' % test_MAE)\n","    scores.append(test_MAE)\n","    trained_models.append(model)\n","    \n","# Evaluation of eDeepTESR\n","\n","evaluation_result = list()\n","\n","for i in range(1, n_splits+1):\n","    n_models_evaluation = partial_evaluation(trained_models, i, testX_usage, testX_seq, testy)\n","    print('> %d: ensemble=%.5f' % (i, n_models_evaluation))\n","    evaluation_result.append(n_models_evaluation)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xd7pdBYQc59w"},"source":["# Load the best model\n","\n","from keras.models import load_model\n","\n","best_models = []\n","\n","for i in range(20):\n","    model_load = load_model('C:/Users/FMB/Documents/python/MLDL/Short_translational_ramp_DL/model_Func_ensemble/Ensemble_reg_set_5(n=20_filter0.5_80000)/model_set_%d.hdf5'%i)\n","    best_models.append(model_load)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"pum6_f0_c59w"},"source":["# Evaluation of eDeepTESR\n","\n","evaluation_result = list()\n","\n","for i in range(1, len(best_models)+1):\n","    n_models_evaluation = partial_evaluation(best_models, i, testX_usage, testX_seq, testy)\n","    print('> %d: ensemble=%.5f' % (i, n_models_evaluation))\n","    evaluation_result.append(n_models_evaluation)\n","\n","\n","# Visualizatino of results from eDeepTESR evaulation\n","\n","x_axis = [i for i in range(1, len(best_models)+1)]\n","plt.figure(figsize = (8,8), dpi = 300)\n","plt.plot(x_axis, evaluation_result)\n","plt.title(\"Evaluation of eDeepTESR\", fontsize = 15)\n","plt.xlabel(\"Number of model training\", fontsize = 10)\n","plt.xticks([0 , 5, 10 , 15, 20])\n","plt.ylabel(\"Mean absolute error\", fontsize = 10)\n","plt.xlim(0, len(best_models)+1)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"XLn61YtWc59x"},"source":["# Visulization of test result from eDeepTESR\n","best_test_Predict = ensemble_predictions(best_models, testX_usage, testX_seq)\n","best_test_loss = partial_evaluation(best_models, 20, testX_usage, testX_seq, testy)\n","\n","plt.figure(figsize = (5,5), dpi = 300)\n","plt.scatter(testy, best_test_Predict, alpha = 0.03)\n","plt.title(\"GFP score(Test set) - TESR score(Prediction)\", fontsize = 10)\n","plt.xlabel(\"GFP score(Test set)\", fontsize = 10)\n","plt.xlim(1.0, 5.0)\n","plt.xticks([1, 2, 3, 4, 5])\n","plt.ylim(1.0, 5.0)\n","plt.yticks([1, 2, 3, 4, 5])\n","plt.ylabel(\"TESR score(Prediction)\", fontsize = 10)\n","plt.show()\n","\n","print(\"Mean absolute error = \", best_test_loss)\n","print(\"r2_score = \", r2_score(testy, best_test_Predict))"],"execution_count":null,"outputs":[]}]}